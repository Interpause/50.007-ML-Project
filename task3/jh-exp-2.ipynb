{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE LATER ###\n",
    "# Go up one directory since notebook inside task3/ folder.\n",
    "import os\n",
    "\n",
    "try:\n",
    "    if UP_DIR:\n",
    "        print(\"skipping\")\n",
    "except NameError:\n",
    "    os.chdir(\"..\")\n",
    "    UP_DIR = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad56939",
   "metadata": {},
   "source": [
    "# JH's Task 3 Experiment 2\n",
    "(NOTE: keep this H1 header block or add it later to denote the boundaries between notebooks when we combined later)\n",
    "\n",
    "Attempting Hyperparameter Sweep using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e8fa9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "from copy import deepcopy\n",
    "from task3.exp2 import Hparams, tfidf_to_np, train, inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d84100",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15db208",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"./data/train.csv\"\n",
    "TRAIN_TFIDF_CSV = \"./data/train_tfidf_features.csv\"\n",
    "TEST_CSV = \"./data/test.csv\"\n",
    "TEST_TFIDF_CSV = \"./data/test_tfidf_features.csv\"\n",
    "HP = Hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36c357",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "Instead of using the tfidf features given in the comp, we can engineer our own tfidf features with better filtering logic, or using something other than tfidf altogether to arrive at vector representations, or perhaps even use a strategy that use non-vector representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d387f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e58864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(TRAIN_CSV, index_col=\"id\")\n",
    "train_tfidf_df = pd.read_csv(TRAIN_TFIDF_CSV, index_col=\"id\")\n",
    "# test_df = pd.read_csv(TEST_CSV, index_col=\"id\")\n",
    "test_tfidf_df = pd.read_csv(TEST_TFIDF_CSV, index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    *tfidf_to_np(train_tfidf_df), test_size=HP.val_split, random_state=HP.seed\n",
    ")\n",
    "test_X, test_y = tfidf_to_np(test_tfidf_df)\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(\"  train:\", len(train_X))\n",
    "print(\"  val:  \", len(val_X))\n",
    "print(\"  test: \", len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbaf306",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Optimization\n",
    "Setting up hyperparameter search using Optuna to find optimal parameters for our XGBoost + PCA pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna optimization.\"\"\"\n",
    "\n",
    "    # Create a copy of base hyperparameters\n",
    "    hp = deepcopy(HP)\n",
    "\n",
    "    hp.pca_n_components = trial.suggest_int(\"pca_n_components\", 200, 800, step=100)\n",
    "    hp.xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 3, 8)\n",
    "    hp.xgb_learning_rate = trial.suggest_float(\n",
    "        \"xgb_learning_rate\", 0.005, 0.1, log=True\n",
    "    )\n",
    "    hp.xgb_subsample = trial.suggest_float(\"xgb_subsample\", 0.5, 1.0)\n",
    "    hp.xgb_colsample_bynode = trial.suggest_float(\"xgb_colsample_bynode\", 0.4, 0.8)\n",
    "    hp.xgb_min_child_weight = trial.suggest_float(\"xgb_min_child_weight\", 1.0, 5.0)\n",
    "    hp.xgb_min_split_loss = trial.suggest_float(\"xgb_min_split_loss\", 0.0, 1.0)\n",
    "    hp.xgb_lambda = trial.suggest_float(\"xgb_lambda\", 0.5, 4.0)\n",
    "    hp.xgb_alpha = trial.suggest_float(\"xgb_alpha\", 0.0, 2.0)\n",
    "\n",
    "    try:\n",
    "        # Train model with suggested hyperparameters\n",
    "        results = train(hp, train_X=train_X, train_y=train_y, val_X=val_X, val_y=val_y, quiet=True)\n",
    "\n",
    "        # Store results\n",
    "        trial.set_user_attr(\"model_xgb\", results[\"model_xgb\"])\n",
    "        trial.set_user_attr(\"model_pca\", results[\"model_pca\"])\n",
    "        trial.set_user_attr(\"hyperparameters\", hp)\n",
    "\n",
    "        # Return val error\n",
    "        return results[\"val_err\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed with error: {e}\")\n",
    "        # Return high value for failed trials\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",  # Minimize val error\n",
    "    # Use TPE sampler for reproducibility\n",
    "    sampler=optuna.samplers.TPESampler(seed=HP.seed),\n",
    ")\n",
    "\n",
    "study.optimize(objective, timeout=60*30, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best validation error: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a05d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimization results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot optimization history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Optimization history\n",
    "trial_numbers = [trial.number for trial in study.trials]\n",
    "trial_values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "trial_nums_valid = [trial.number for trial in study.trials if trial.value is not None]\n",
    "\n",
    "ax1.plot(trial_nums_valid, trial_values, \"b-\", alpha=0.7)\n",
    "ax1.axhline(\n",
    "    y=min(trial_values),\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Best: {min(trial_values):.4f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Trial Number\")\n",
    "ax1.set_ylabel(\"Validation Error\")\n",
    "ax1.set_title(\"Optimization History\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter importance (if enough trials)\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "params = list(importance.keys())\n",
    "values = list(importance.values())\n",
    "\n",
    "ax2.barh(params, values)\n",
    "ax2.set_xlabel(\"Importance\")\n",
    "ax2.set_title(\"Hyperparameter Importance\")\n",
    "ax2.tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best trial details\n",
    "print(\"\\\\nBest Trial Details:\")\n",
    "print(f\"Trial number: {study.best_trial.number}\")\n",
    "print(f\"Validation error: {study.best_trial.value:.4f}\")\n",
    "print(\"Parameters:\")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "best_results = {\n",
    "    \"model_xgb\": best_trial.user_attrs[\"model_xgb\"],\n",
    "    \"model_pca\": best_trial.user_attrs[\"model_pca\"],\n",
    "    \"val_err\": best_trial.value,\n",
    "}\n",
    "best_hp = best_trial.user_attrs[\"hyperparameters\"]\n",
    "\n",
    "print(f\"Best model validation error: {best_results['val_err']:.4f}\")\n",
    "print(\"Optimal hyperparameters:\")\n",
    "print(f\"  PCA components: {best_hp.pca_n_components}\")\n",
    "print(f\"  XGBoost max_depth: {best_hp.xgb_max_depth}\")\n",
    "print(f\"  XGBoost learning_rate: {best_hp.xgb_learning_rate:.4f}\")\n",
    "print(f\"  XGBoost subsample: {best_hp.xgb_subsample:.3f}\")\n",
    "print(f\"  XGBoost colsample_bynode: {best_hp.xgb_colsample_bynode:.3f}\")\n",
    "print(f\"  XGBoost min_child_weight: {best_hp.xgb_min_child_weight:.2f}\")\n",
    "print(f\"  XGBoost min_split_loss: {best_hp.xgb_min_split_loss:.3f}\")\n",
    "print(f\"  XGBoost lambda: {best_hp.xgb_lambda:.3f}\")\n",
    "print(f\"  XGBoost alpha: {best_hp.xgb_alpha:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee060da2",
   "metadata": {},
   "source": [
    "## Inference with Optimized Model\n",
    "Using the best hyperparameters found by Optuna optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ba992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = inference(\n",
    "    test_X, model_xgb=best_results[\"model_xgb\"], model_pca=best_results[\"model_pca\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "pred_df = pd.DataFrame(\n",
    "    zip(test_tfidf_df.index, np.where(pred_y > 0.5, 1, 0)), columns=[\"row ID\", \"label\"]\n",
    ")\n",
    "# YYYYMMDD-HHMM-optuna.csv\n",
    "pred_df.to_csv(f\"{datetime.now().strftime('%Y%m%d-%H%M')}-optuna.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b683d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007-ML-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
